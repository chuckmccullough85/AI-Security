{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisoning Attacks\n",
    "\n",
    "In this lab, we will use the MNIST dataset to train an SVM model to classify handwritten digits. Then we will use a poisoning attack to add a backdoor to the model. The backdoor will cause the model to misclassify any image that contains a specific trigger pattern.\n",
    "\n",
    "First, get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import secml\n",
    "\n",
    "from secml.data.loader import CDataLoaderMNIST\n",
    "\n",
    "# MNIST dataset will be downloaded and cached if needed\n",
    "loader = CDataLoaderMNIST()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now, let's train the model only for digits 5 and 9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 999\n",
    "\n",
    "n_tr = 100  # Number of training set samples\n",
    "n_val = 500  # Number of validation set samples\n",
    "n_ts = 500  # Number of test set samples\n",
    "\n",
    "digits = (5, 9)\n",
    "\n",
    "tr_val = loader.load('training', digits=digits, num_samples=n_tr + n_val)\n",
    "ts = loader.load('testing', digits=digits, num_samples=n_ts)\n",
    "\n",
    "# Split in training and validation set\n",
    "tr = tr_val[:n_tr, :]\n",
    "val = tr_val[n_tr:, :]\n",
    "\n",
    "# Normalize the features in `[0, 1]`\n",
    "tr.X /= 255\n",
    "val.X /= 255\n",
    "ts.X /= 255\n",
    "\n",
    "from secml.ml.classifiers import CClassifierSVM\n",
    "# train SVM in the dual space, on a linear kernel, as needed for poisoning\n",
    "clf = CClassifierSVM(C=10, kernel='linear')\n",
    "\n",
    "print(\"Training of classifier...\")\n",
    "clf.fit(tr.X, tr.Y)\n",
    "\n",
    "# Compute predictions on a test set\n",
    "y_pred = clf.predict(ts.X)\n",
    "\n",
    "# Metric to use for performance evaluation\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy on test set: {:.2%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The poison attack\n",
    "The parameters for poisoning attack are much simpler than the evasion attack.\n",
    "\n",
    "We will specify 15 poisoning points.\n",
    "\n",
    "First - define a function to show the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secml.figure import CFigure\n",
    "# Only required for visualization in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Let's define a convenience function to easily plot the MNIST dataset\n",
    "def show_digits(samples, preds, labels, digs, n_display=8):\n",
    "    samples = samples.atleast_2d()\n",
    "    n_display = min(n_display, samples.shape[0])\n",
    "    fig = CFigure(width=n_display*2, height=3)\n",
    "    for idx in range(n_display):\n",
    "        fig.subplot(2, n_display, idx+1)\n",
    "        fig.sp.xticks([])\n",
    "        fig.sp.yticks([])\n",
    "        fig.sp.imshow(samples[idx, :].reshape((28, 28)), cmap='gray')\n",
    "        fig.sp.title(\"{} ({})\".format(digits[labels[idx].item()], digs[preds[idx].item()]),\n",
    "                     color=(\"green\" if labels[idx].item()==preds[idx].item() else \"red\"))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create the poisoning points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb, ub = 0., 1.  # Bounds of the attack space. Can be set to `None` for unbounded\n",
    "n_poisoning_points = 15  # Number of poisoning points to generate\n",
    "\n",
    "# Should be chosen depending on the optimization problem\n",
    "solver_params = {\n",
    "    'eta': 0.25,\n",
    "    'eta_min': 2.0,\n",
    "    'eta_max': None,\n",
    "    'max_iter': 100,\n",
    "    'eps': 1e-6\n",
    "}\n",
    "\n",
    "from secml.adv.attacks import CAttackPoisoningSVM\n",
    "pois_attack = CAttackPoisoningSVM(classifier=clf,\n",
    "                                  training_data=tr,\n",
    "                                  val=val,\n",
    "                                  lb=lb, ub=ub,\n",
    "                                  solver_params=solver_params,\n",
    "                                  random_seed=random_state)\n",
    "pois_attack.n_points = n_poisoning_points\n",
    "\n",
    "# Run the poisoning attack\n",
    "print(\"Attack started...\")\n",
    "pois_y_pred, _, pois_points_ds, _ = pois_attack.run(ts.X, ts.Y)\n",
    "print(\"Attack complete!\")\n",
    "\n",
    "# Evaluate the accuracy of the original classifier\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=clf.predict(ts.X))\n",
    "# Evaluate the accuracy after the poisoning attack\n",
    "pois_acc = metric.performance_score(y_true=ts.Y, y_pred=pois_y_pred)\n",
    "\n",
    "print(\"Original accuracy on test set: {:.2%}\".format(acc))\n",
    "print(\"Accuracy after attack on test set: {:.2%}\".format(pois_acc))\n",
    "\n",
    "# Training of the poisoned classifier for visualization purposes\n",
    "pois_clf = clf.deepcopy()\n",
    "pois_tr = tr.append(pois_points_ds)  # Join the training set with the poisoning points\n",
    "pois_clf.fit(pois_tr.X, pois_tr.Y)\n",
    "\n",
    "show_digits(pois_points_ds.X, pois_clf.predict(pois_points_ds.X), \n",
    "            pois_points_ds.Y, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that by poisioning the model, the accuracy of the model will drop significantly.\n",
    "\n",
    "Let's note that the label of each adversarial example we show has been \n",
    " *flipped* by the attack with respect to the actual true label. Thus, the \n",
    " predicted label (parenthesis) by the poisoned classifier is displayed \n",
    " in green when *different* from the true label of the digit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
